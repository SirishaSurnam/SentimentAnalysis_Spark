{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNIBVO5H1VsgyxvKZNdypk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SirishaSurnam/SentimentAnalysis_Spark/blob/main/BDA_Sentiment140_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter Data Analysis using Apache Spark on Cloud"
      ],
      "metadata": {
        "id": "ZdpTUsLXLm_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf spark-4.0.1-bin-hadoop3*"
      ],
      "metadata": {
        "id": "4aP9SHXfllfU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ********************************** Spark 2.3.x Env Setup For Google Collab ********************************** #\n",
        "\n",
        "# download spark 3.5\n",
        "!wget -q -O spark-3.5.0-bin-hadoop3.tgz https://downloads.apache.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# extract spark archive\n",
        "!tar xf spark-3.5.0-bin-hadoop3.tgz\n",
        "\n",
        "# install findspark\n",
        "!pip install -q findspark\n",
        "\n",
        "# ********************************** end of setup ********************************** #\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4DP-UQgkYAO",
        "outputId": "2324337a-b125-463e-9c42-312f007a7150"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tar: This does not look like a tar archive\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Child returned status 1\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDYugNsE86VU",
        "outputId": "e309900d-7cd7-425b-e17c-eb17faedee5c"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8.0K\n",
            "drwx------ 5 root root 4.0K Sep 16 12:16 drive\n",
            "drwxr-xr-x 1 root root 4.0K Sep  9 13:46 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"App_Name\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# set the log level to WARN\n",
        "spark.sparkContext.setLogLevel('WARN')\n",
        "\n",
        "print(\"✅ Spark session created successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ax0TfvuboSX7",
        "outputId": "b36cf20e-a175-4c4e-e6d7-c1222b1078ce"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Spark session created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F-ehLYg3rW6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment140 Dataset Analysis using Apache Spark\n",
        "# Dataset: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import re"
      ],
      "metadata": {
        "id": "VEeAWwfJxIwA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"training.1600000.processed.noemoticon.csv\"\n"
      ],
      "metadata": {
        "id": "L5OGZUmAxWe9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Sentiment140Analyzer:\n",
        "    def __init__(self, app_name=\"Sentiment140Analysis\"):\n",
        "        \"\"\"Initialize Spark session for Sentiment140 dataset analysis\"\"\"\n",
        "        self.spark = SparkSession.builder \\\n",
        "            .appName(app_name) \\\n",
        "            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "            .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
        "            .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "            .getOrCreate()\n",
        "\n",
        "        self.spark.sparkContext.setLogLevel(\"WARN\")\n",
        "        self.clean_text_udf = self.create_text_cleaning_udf()\n",
        "\n",
        "\n",
        "    def define_sentiment140_schema(self):\n",
        "        \"\"\"Define schema for Sentiment140 dataset\"\"\"\n",
        "        return StructType([\n",
        "            StructField(\"target\", IntegerType(), True),      # Sentiment label (0=negative, 4=positive)\n",
        "            StructField(\"ids\", StringType(), True),          # Tweet ID\n",
        "            StructField(\"date\", StringType(), True),         # Date of tweet\n",
        "            StructField(\"flag\", StringType(), True),         # Query flag\n",
        "            StructField(\"user\", StringType(), True),         # Username\n",
        "            StructField(\"text\", StringType(), True)          # Tweet text\n",
        "        ])\n",
        "\n",
        "    def load_sentiment140_dataset(self, file_path):\n",
        "        \"\"\"Load Sentiment140 dataset from CSV file\"\"\"\n",
        "        schema = self.define_sentiment140_schema()\n",
        "\n",
        "        # Load the CSV file (no headers in original dataset)\n",
        "        df = self.spark.read.csv(file_path, schema=schema, encoding='latin1')\n",
        "\n",
        "        print(f\"Dataset loaded successfully!\")\n",
        "        print(f\"Total records: {df.count()}\")\n",
        "        df.show(5)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def preprocess_sentiment140_data(self, df):\n",
        "        \"\"\"Preprocess Sentiment140 dataset\"\"\"\n",
        "        print(\"Preprocessing Sentiment140 data...\")\n",
        "\n",
        "        # Convert sentiment labels: 0 -> 0 (negative), 4 -> 1 (positive)\n",
        "        df = df.withColumn(\"sentiment_label\",\n",
        "                          when(col(\"target\") == 0, 0)  # Negative\n",
        "                          .when(col(\"target\") == 4, 1)  # Positive\n",
        "                          .otherwise(2))  # Neutral (though not in original dataset)\n",
        "\n",
        "        # Convert date to proper timestamp\n",
        "        df = df.withColumn(\"timestamp\", to_timestamp(col(\"date\"), \"EEE MMM dd HH:mm:ss Z yyyy\"))\n",
        "\n",
        "        # Extract date components for analysis\n",
        "        df = df.withColumn(\"year\", year(\"timestamp\")) \\\n",
        "               .withColumn(\"month\", month(\"timestamp\")) \\\n",
        "               .withColumn(\"day\", dayofmonth(\"timestamp\")) \\\n",
        "               .withColumn(\"hour\", hour(\"timestamp\"))\n",
        "\n",
        "        # Clean tweet text\n",
        "        df = df.withColumn(\"cleaned_text\", self.clean_text_udf(col(\"text\")))\n",
        "\n",
        "        # Filter out empty tweets after cleaning\n",
        "        df = df.filter(col(\"cleaned_text\").isNotNull() & (length(col(\"cleaned_text\")) > 10))\n",
        "\n",
        "        # Add sentiment labels as string for better readability\n",
        "        df = df.withColumn(\"sentiment\",\n",
        "                          when(col(\"sentiment_label\") == 0, \"negative\")\n",
        "                          .when(col(\"sentiment_label\") == 1, \"positive\")\n",
        "                          .otherwise(\"neutral\"))\n",
        "\n",
        "        print(f\"Data preprocessed. Records after cleaning: {df.count()}\")\n",
        "        return df\n",
        "\n",
        "    def create_text_cleaning_udf(self):\n",
        "        \"\"\"Create UDF for text cleaning\"\"\"\n",
        "        def clean_text(text):\n",
        "            if text is None:\n",
        "                return None\n",
        "\n",
        "            # Convert to lowercase\n",
        "            text = text.lower()\n",
        "\n",
        "            # Remove URLs\n",
        "            text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "            # Remove user mentions and hashtags symbols\n",
        "            text = re.sub(r'@\\w+|#', '', text)\n",
        "\n",
        "            # Remove extra whitespace\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            # Remove non-alphabetic characters (keep spaces)\n",
        "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "            return text if len(text.strip()) > 0 else None\n",
        "\n",
        "        return udf(clean_text, StringType())\n",
        "\n",
        "\n",
        "    def exploratory_data_analysis(self, df):\n",
        "        \"\"\"Perform comprehensive EDA on Sentiment140 dataset\"\"\"\n",
        "        print(\"\\n=== EXPLORATORY DATA ANALYSIS ===\")\n",
        "\n",
        "        # Basic statistics\n",
        "        print(\"1. Dataset Overview:\")\n",
        "        print(f\"Total tweets: {df.count():,}\")\n",
        "        print(f\"Unique users: {df.select('user').distinct().count():,}\")\n",
        "\n",
        "        # Sentiment distribution\n",
        "        print(\"\\n2. Sentiment Distribution:\")\n",
        "        sentiment_dist = df.groupBy(\"sentiment\").count().orderBy(\"count\", ascending=False)\n",
        "        sentiment_dist.show()\n",
        "\n",
        "        # Store for visualization\n",
        "        sentiment_pandas = sentiment_dist.toPandas()\n",
        "\n",
        "        # Temporal analysis\n",
        "        print(\"\\n3. Temporal Distribution:\")\n",
        "        temporal_dist = df.groupBy(\"year\", \"month\").count().orderBy(\"year\", \"month\")\n",
        "        temporal_dist.show(20)\n",
        "\n",
        "        # Hourly patterns\n",
        "        print(\"\\n4. Hourly Tweet Patterns:\")\n",
        "        hourly_dist = df.groupBy(\"hour\").count().orderBy(\"hour\")\n",
        "        hourly_dist.show(24)\n",
        "\n",
        "        # Top users by tweet count\n",
        "        print(\"\\n5. Most Active Users:\")\n",
        "        top_users = df.groupBy(\"user\").count().orderBy(desc(\"count\")).limit(10)\n",
        "        top_users.show()\n",
        "\n",
        "        # Text length analysis\n",
        "        print(\"\\n6. Text Length Statistics:\")\n",
        "        text_stats = df.select(length(\"cleaned_text\").alias(\"text_length\")) \\\n",
        "                      .describe(\"text_length\")\n",
        "        text_stats.show()\n",
        "\n",
        "        # Average text length by sentiment\n",
        "        print(\"\\n7. Average Text Length by Sentiment:\")\n",
        "        avg_length = df.groupBy(\"sentiment\") \\\n",
        "                      .agg(avg(length(\"cleaned_text\")).alias(\"avg_text_length\")) \\\n",
        "                      .orderBy(\"avg_text_length\", ascending=False)\n",
        "        avg_length.show()\n",
        "\n",
        "        return {\n",
        "            'sentiment_dist': sentiment_pandas,\n",
        "            'temporal_dist': temporal_dist.toPandas(),\n",
        "            'hourly_dist': hourly_dist.toPandas()\n",
        "        }\n",
        "\n",
        "    def word_frequency_analysis(self, df):\n",
        "        \"\"\"Analyze word frequencies by sentiment\"\"\"\n",
        "        print(\"\\n=== WORD FREQUENCY ANALYSIS ===\")\n",
        "\n",
        "        # Tokenize words\n",
        "        tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
        "        words_df = tokenizer.transform(df)\n",
        "\n",
        "        # Remove stop words\n",
        "        remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "        filtered_df = remover.transform(words_df)\n",
        "\n",
        "        # Explode words and analyze frequency by sentiment\n",
        "        exploded_df = filtered_df.select(\"sentiment\", explode(\"filtered_words\").alias(\"word\"))\n",
        "\n",
        "        # Overall word frequency\n",
        "        print(\"Top 20 Most Common Words:\")\n",
        "        word_freq = exploded_df.groupBy(\"word\").count() \\\n",
        "                              .filter(length(\"word\") > 2) \\\n",
        "                              .orderBy(desc(\"count\")) \\\n",
        "                              .limit(20)\n",
        "        word_freq.show()\n",
        "\n",
        "        # Word frequency by sentiment\n",
        "        print(\"Top Words by Sentiment - Positive:\")\n",
        "        positive_words = exploded_df.filter(col(\"sentiment\") == \"positive\") \\\n",
        "                                   .groupBy(\"word\").count() \\\n",
        "                                   .filter(length(\"word\") > 2) \\\n",
        "                                   .orderBy(desc(\"count\")) \\\n",
        "                                   .limit(15)\n",
        "        positive_words.show()\n",
        "\n",
        "        print(\"Top Words by Sentiment - Negative:\")\n",
        "        negative_words = exploded_df.filter(col(\"sentiment\") == \"negative\") \\\n",
        "                                   .groupBy(\"word\").count() \\\n",
        "                                   .filter(length(\"word\") > 2) \\\n",
        "                                   .orderBy(desc(\"count\")) \\\n",
        "                                   .limit(15)\n",
        "        negative_words.show()\n",
        "\n",
        "        return {\n",
        "            'overall_words': word_freq.toPandas(),\n",
        "            'positive_words': positive_words.toPandas(),\n",
        "            'negative_words': negative_words.toPandas()\n",
        "        }\n",
        "\n",
        "    def build_sentiment_classifier(self, df):\n",
        "        \"\"\"Build and train sentiment classification models\"\"\"\n",
        "        print(\"\\n=== BUILDING SENTIMENT CLASSIFIER ===\")\n",
        "\n",
        "        # Prepare features\n",
        "        tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
        "        remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "        hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=2000)\n",
        "        idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
        "\n",
        "        # Reduce data\n",
        "        df = df.sample(0.1, seed=42)\n",
        "        # Split data\n",
        "        train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "        print(f\"Training set: {train_data.count():,} tweets\")\n",
        "        print(f\"Test set: {test_data.count():,} tweets\")\n",
        "\n",
        "        # Logistic Regression Pipeline\n",
        "        print(\"\\n1. Training Logistic Regression Model...\")\n",
        "        lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment_label\", maxIter=30)\n",
        "        lr_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
        "\n",
        "        lr_model = lr_pipeline.fit(train_data)\n",
        "        lr_predictions = lr_model.transform(test_data)\n",
        "\n",
        "        # Evaluate Logistic Regression\n",
        "        evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\",\n",
        "                                                    predictionCol=\"prediction\",\n",
        "                                                    metricName=\"accuracy\")\n",
        "        lr_accuracy = evaluator.evaluate(lr_predictions)\n",
        "\n",
        "        '''\n",
        "        # Random Forest Pipeline\n",
        "        print(\"2. Training Random Forest Model...\")\n",
        "        rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"sentiment_label\", numTrees=50)\n",
        "        rf_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, rf])\n",
        "\n",
        "        rf_model = rf_pipeline.fit(train_data)\n",
        "        rf_predictions = rf_model.transform(test_data)\n",
        "        rf_accuracy = evaluator.evaluate(rf_predictions)\n",
        "        '''\n",
        "\n",
        "        print(f\"\\nModel Performance:\")\n",
        "        print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n",
        "        # print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "\n",
        "        '''\n",
        "        # Detailed evaluation for best model\n",
        "        best_model = lr_model if lr_accuracy > rf_accuracy else rf_model\n",
        "        best_predictions = lr_predictions if lr_accuracy > rf_accuracy else rf_predictions\n",
        "        best_name = \"Logistic Regression\" if lr_accuracy > rf_accuracy else \"Random Forest\"\n",
        "        '''\n",
        "        best_model = lr_model\n",
        "        best_predictions = lr_predictions\n",
        "        best_name = \"Logistic Regression\"\n",
        "\n",
        "        print(f\"\\nDetailed evaluation for {best_name}:\")\n",
        "\n",
        "        # Confusion Matrix\n",
        "        confusion_matrix = best_predictions.groupBy(\"sentiment_label\", \"prediction\").count()\n",
        "        confusion_matrix.show()\n",
        "\n",
        "        # Precision, Recall, F1-Score\n",
        "        precision_evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\",\n",
        "                                                               predictionCol=\"prediction\",\n",
        "                                                               metricName=\"weightedPrecision\")\n",
        "        recall_evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\",\n",
        "                                                            predictionCol=\"prediction\",\n",
        "                                                            metricName=\"weightedRecall\")\n",
        "        f1_evaluator = MulticlassClassificationEvaluator(labelCol=\"sentiment_label\",\n",
        "                                                        predictionCol=\"prediction\",\n",
        "                                                        metricName=\"f1\")\n",
        "\n",
        "        precision = precision_evaluator.evaluate(best_predictions)\n",
        "        recall = recall_evaluator.evaluate(best_predictions)\n",
        "        f1_score = f1_evaluator.evaluate(best_predictions)\n",
        "\n",
        "        print(f\"Weighted Precision: {precision:.4f}\")\n",
        "        print(f\"Weighted Recall: {recall:.4f}\")\n",
        "        print(f\"F1-Score: {f1_score:.4f}\")\n",
        "\n",
        "        #  'accuracy': lr_accuracy if lr_accuracy > rf_accuracy else rf_accuracy,\n",
        "\n",
        "        return {\n",
        "            'best_model': best_model,\n",
        "            'best_predictions': best_predictions,\n",
        "            'model_name': best_name,\n",
        "            'accuracy': lr_accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1_score,\n",
        "            'confusion_matrix': confusion_matrix.toPandas()\n",
        "        }\n",
        "\n",
        "    def advanced_sentiment_analysis(self, df):\n",
        "        \"\"\"Advanced sentiment analysis with custom insights\"\"\"\n",
        "        print(\"\\n=== ADVANCED SENTIMENT ANALYSIS ===\")\n",
        "\n",
        "        # Sentiment by time patterns\n",
        "        print(\"1. Sentiment Patterns by Hour:\")\n",
        "        hourly_sentiment = df.groupBy(\"hour\", \"sentiment\").count() \\\n",
        "                            .orderBy(\"hour\", \"sentiment\")\n",
        "        hourly_sentiment.show(50)\n",
        "\n",
        "        # User sentiment consistency\n",
        "        print(\"2. User Sentiment Consistency:\")\n",
        "        user_sentiment = df.groupBy(\"user\", \"sentiment\").count()\n",
        "        user_variety = user_sentiment.groupBy(\"user\") \\\n",
        "                                   .agg(countDistinct(\"sentiment\").alias(\"sentiment_types\"),\n",
        "                                        sum(\"count\").alias(\"total_tweets\")) \\\n",
        "                                   .filter(col(\"total_tweets\") >= 5) \\\n",
        "                                   .orderBy(desc(\"total_tweets\"))\n",
        "\n",
        "        print(\"Users with most diverse sentiment (both positive and negative):\")\n",
        "        diverse_users = user_variety.filter(col(\"sentiment_types\") >= 2).limit(10)\n",
        "        diverse_users.show()\n",
        "\n",
        "        # Long vs short tweets sentiment\n",
        "        print(\"3. Sentiment by Tweet Length:\")\n",
        "        df_with_length = df.withColumn(\"text_length\", length(\"cleaned_text\")) \\\n",
        "                          .withColumn(\"length_category\",\n",
        "                                     when(col(\"text_length\") < 50, \"short\")\n",
        "                                     .when(col(\"text_length\") < 100, \"medium\")\n",
        "                                     .otherwise(\"long\"))\n",
        "\n",
        "        length_sentiment = df_with_length.groupBy(\"length_category\", \"sentiment\").count() \\\n",
        "                                        .orderBy(\"length_category\", \"sentiment\")\n",
        "        length_sentiment.show()\n",
        "\n",
        "        return {\n",
        "            'hourly_sentiment': hourly_sentiment.toPandas(),\n",
        "            'user_variety': user_variety.toPandas(),\n",
        "            'length_sentiment': length_sentiment.toPandas()\n",
        "        }\n",
        "\n",
        "    def predict_new_tweets(self, model, new_texts):\n",
        "        \"\"\"Predict sentiment for new tweets using trained model\"\"\"\n",
        "        print(\"\\n=== PREDICTING NEW TWEETS ===\")\n",
        "\n",
        "        # Create DataFrame with new texts\n",
        "        new_data = self.spark.createDataFrame([(text,) for text in new_texts], [\"text\"])\n",
        "        new_data = new_data.withColumn(\"cleaned_text\", self.clean_text_udf(col(\"text\")))\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = model.transform(new_data)\n",
        "\n",
        "        # Show results\n",
        "        results = predictions.select(\"text\", \"prediction\",\n",
        "                                   when(col(\"prediction\") == 0, \"negative\")\n",
        "                                   .when(col(\"prediction\") == 1, \"positive\")\n",
        "                                   .otherwise(\"neutral\").alias(\"predicted_sentiment\"))\n",
        "\n",
        "        print(\"Predictions for new tweets:\")\n",
        "        results.show(truncate=False)\n",
        "\n",
        "        return results.toPandas()\n",
        "\n",
        "    def generate_insights_report(self, df, analysis_results):\n",
        "        \"\"\"Generate comprehensive insights report\"\"\"\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"SENTIMENT140 DATASET ANALYSIS REPORT\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        total_tweets = df.count()\n",
        "        unique_users = df.select(\"user\").distinct().count()\n",
        "\n",
        "        print(f\"\\nDATASET OVERVIEW:\")\n",
        "        print(f\"• Total Tweets Analyzed: {total_tweets:,}\")\n",
        "        print(f\"• Unique Users: {unique_users:,}\")\n",
        "        print(f\"• Average Tweets per User: {total_tweets/unique_users:.1f}\")\n",
        "\n",
        "        # Sentiment insights\n",
        "        sentiment_dist = df.groupBy(\"sentiment\").count().collect()\n",
        "        sentiment_dict = {row['sentiment']: row['count'] for row in sentiment_dist}\n",
        "\n",
        "        print(f\"\\nSENTIMENT DISTRIBUTION:\")\n",
        "        for sentiment, count in sentiment_dict.items():\n",
        "            percentage = (count / total_tweets) * 100\n",
        "            print(f\"• {sentiment.title()}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "        # Time insights\n",
        "        peak_hour = df.groupBy(\"hour\").count().orderBy(desc(\"count\")).first()\n",
        "        print(f\"\\nTEMPORAL INSIGHTS:\")\n",
        "        print(f\"• Peak Activity Hour: {peak_hour['hour']:02d}:00 ({peak_hour['count']:,} tweets)\")\n",
        "\n",
        "        # Model performance\n",
        "        if 'accuracy' in analysis_results:\n",
        "            print(f\"\\nMODEL PERFORMANCE:\")\n",
        "            print(f\"• Best Model: {analysis_results['model_name']}\")\n",
        "            print(f\"• Accuracy: {analysis_results['accuracy']:.1%}\")\n",
        "            print(f\"• F1-Score: {analysis_results['f1_score']:.3f}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "    def visualize_results(self, analysis_data):\n",
        "        \"\"\"Create visualizations for the analysis results\"\"\"\n",
        "        print(\"\\nGenerating visualizations...\")\n",
        "\n",
        "        # Sentiment distribution pie chart\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Subplot 1: Sentiment Distribution\n",
        "        plt.subplot(2, 3, 1)\n",
        "        sentiment_data = analysis_data['sentiment_dist']\n",
        "        plt.pie(sentiment_data['count'], labels=sentiment_data['sentiment'], autopct='%1.1f%%')\n",
        "        plt.title('Sentiment Distribution')\n",
        "\n",
        "        # Subplot 2: Hourly Tweet Patterns\n",
        "        plt.subplot(2, 3, 2)\n",
        "        hourly_data = analysis_data['hourly_dist']\n",
        "        plt.bar(hourly_data['hour'], hourly_data['count'])\n",
        "        plt.title('Tweets by Hour of Day')\n",
        "        plt.xlabel('Hour')\n",
        "        plt.ylabel('Tweet Count')\n",
        "\n",
        "        # Subplot 3: Word Frequency\n",
        "        plt.subplot(2, 3, 3)\n",
        "        if 'overall_words' in analysis_data:\n",
        "            words_data = analysis_data['overall_words'].head(10)\n",
        "            plt.barh(words_data['word'], words_data['count'])\n",
        "            plt.title('Top 10 Most Common Words')\n",
        "            plt.xlabel('Frequency')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('sentiment140_analysis.png', dpi=300, bbox_inches='tight')\n",
        "        print(\"Visualizations saved as 'sentiment140_analysis.png'\")\n",
        "\n",
        "    def save_results(self, df, results, output_path):\n",
        "        \"\"\"Save analysis results to files\"\"\"\n",
        "        print(f\"\\nSaving results to {output_path}...\")\n",
        "\n",
        "        # Save processed dataset\n",
        "        df.write.mode(\"overwrite\").parquet(f\"{output_path}/processed_sentiment140.parquet\")\n",
        "\n",
        "        # Save analysis results as JSON\n",
        "        import json\n",
        "        results_json = {}\n",
        "        for key, value in results.items():\n",
        "            if hasattr(value, 'to_dict'):\n",
        "                results_json[key] = value.to_dict()\n",
        "            elif isinstance(value, (int, float, str)):\n",
        "                results_json[key] = value\n",
        "\n",
        "        with open(f\"{output_path}/analysis_results.json\", 'w') as f:\n",
        "            json.dump(results_json, f, indent=2)\n",
        "\n",
        "        print(\"Results saved successfully!\")\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close Spark session\"\"\"\n",
        "        self.spark.stop()"
      ],
      "metadata": {
        "id": "RSgnX4ZtuiaG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Data Loading and Preprocessing\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "print(\"Starting Sentiment140 Dataset Analysis with Apache Spark\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize analyzer\n",
        "analyzer = Sentiment140Analyzer()\n",
        "\n",
        "# Step 0: Download the dataset using kagglehub\n",
        "print(\"Downloading dataset from Kaggle...\")\n",
        "download_path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "dataset_path = os.path.join(download_path, \"training.1600000.processed.noemoticon.csv\")\n",
        "\n",
        "# Configuration\n",
        "output_path = \"sentiment140_results\"\n",
        "\n",
        "# Step 1: Load dataset\n",
        "print(f\"\\nStep 1: Loading Sentiment140 dataset from {dataset_path}...\")\n",
        "df = analyzer.load_sentiment140_dataset(dataset_path)\n",
        "\n",
        "# Step 2: Preprocess data\n",
        "print(\"\\nStep 2: Preprocessing data...\")\n",
        "df = analyzer.preprocess_sentiment140_data(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cp9UsLm02Iaq",
        "outputId": "7c32db36-db89-4adc-8f84-6bf714aef4fe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Sentiment140 Dataset Analysis with Apache Spark\n",
            "============================================================\n",
            "Downloading dataset from Kaggle...\n",
            "Using Colab cache for faster access to the 'sentiment140' dataset.\n",
            "\n",
            "Step 1: Loading Sentiment140 dataset from /kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv...\n",
            "Dataset loaded successfully!\n",
            "Total records: 1600000\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|target|       ids|                date|    flag|           user|                text|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "|     0|1467810369|Mon Apr 06 22:19:...|NO_QUERY|_TheSpecialOne_|@switchfoot http:...|\n",
            "|     0|1467810672|Mon Apr 06 22:19:...|NO_QUERY|  scotthamilton|is upset that he ...|\n",
            "|     0|1467810917|Mon Apr 06 22:19:...|NO_QUERY|       mattycus|@Kenichan I dived...|\n",
            "|     0|1467811184|Mon Apr 06 22:19:...|NO_QUERY|        ElleCTF|my whole body fee...|\n",
            "|     0|1467811193|Mon Apr 06 22:19:...|NO_QUERY|         Karoli|@nationwideclass ...|\n",
            "+------+----------+--------------------+--------+---------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Step 2: Preprocessing data...\n",
            "Preprocessing Sentiment140 data...\n",
            "Data preprocessed. Records after cleaning: 1559073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 4: EDA and Word Frequency Analysis\n",
        "\n",
        "print(\"\\nStep 3: Performing Exploratory Data Analysis...\")\n",
        "eda_results = analyzer.exploratory_data_analysis(df)\n",
        "\n",
        "print(\"\\nStep 4: Analyzing word frequencies...\")\n",
        "word_analysis = analyzer.word_frequency_analysis(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-nXUAIq2IXQ",
        "outputId": "686d1233-d8a4-46c8-d2f1-440605faa744"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 3: Performing Exploratory Data Analysis...\n",
            "\n",
            "=== EXPLORATORY DATA ANALYSIS ===\n",
            "1. Dataset Overview:\n",
            "Total tweets: 1,559,073\n",
            "Unique users: 649,936\n",
            "\n",
            "2. Sentiment Distribution:\n",
            "+---------+------+\n",
            "|sentiment| count|\n",
            "+---------+------+\n",
            "| negative|783531|\n",
            "| positive|775542|\n",
            "+---------+------+\n",
            "\n",
            "\n",
            "3. Temporal Distribution:\n",
            "+----+-----+-------+\n",
            "|year|month|  count|\n",
            "+----+-----+-------+\n",
            "|NULL| NULL|1559073|\n",
            "+----+-----+-------+\n",
            "\n",
            "\n",
            "4. Hourly Tweet Patterns:\n",
            "+----+-------+\n",
            "|hour|  count|\n",
            "+----+-------+\n",
            "|NULL|1559073|\n",
            "+----+-------+\n",
            "\n",
            "\n",
            "5. Most Active Users:\n",
            "+---------------+-----+\n",
            "|           user|count|\n",
            "+---------------+-----+\n",
            "|       lost_dog|  549|\n",
            "|        webwoke|  345|\n",
            "|    VioletsCRUK|  276|\n",
            "|SallytheShizzle|  272|\n",
            "|    mcraddictal|  259|\n",
            "|       tsarnick|  246|\n",
            "|    what_bugs_u|  246|\n",
            "|      DarkPiano|  226|\n",
            "|   SongoftheOss|  220|\n",
            "|      Jayme1988|  217|\n",
            "+---------------+-----+\n",
            "\n",
            "\n",
            "6. Text Length Statistics:\n",
            "+-------+------------------+\n",
            "|summary|       text_length|\n",
            "+-------+------------------+\n",
            "|  count|           1559073|\n",
            "|   mean| 63.56268115732875|\n",
            "| stddev|33.302942876209976|\n",
            "|    min|                11|\n",
            "|    max|               178|\n",
            "+-------+------------------+\n",
            "\n",
            "\n",
            "7. Average Text Length by Sentiment:\n",
            "+---------+-----------------+\n",
            "|sentiment|  avg_text_length|\n",
            "+---------+-----------------+\n",
            "| negative|65.18202342983238|\n",
            "| positive|61.92665774387461|\n",
            "+---------+-----------------+\n",
            "\n",
            "\n",
            "Step 4: Analyzing word frequencies...\n",
            "\n",
            "=== WORD FREQUENCY ANALYSIS ===\n",
            "Top 20 Most Common Words:\n",
            "+------+-----+\n",
            "|  word|count|\n",
            "+------+-----+\n",
            "|  good|88503|\n",
            "|   day|84448|\n",
            "|   get|81580|\n",
            "|  like|77674|\n",
            "|  dont|67241|\n",
            "| today|64566|\n",
            "| going|64028|\n",
            "|  love|63096|\n",
            "|  cant|62754|\n",
            "|  work|62121|\n",
            "|   got|60794|\n",
            "|  time|55913|\n",
            "|  back|55779|\n",
            "|   lol|54737|\n",
            "|   one|52100|\n",
            "|  know|50650|\n",
            "|really|49459|\n",
            "|   see|45736|\n",
            "|  well|44315|\n",
            "| still|42770|\n",
            "+------+-----+\n",
            "\n",
            "Top Words by Sentiment - Positive:\n",
            "+------+-----+\n",
            "|  word|count|\n",
            "+------+-----+\n",
            "|  good|60004|\n",
            "|  love|46519|\n",
            "|   day|45169|\n",
            "|  like|37021|\n",
            "|   get|36207|\n",
            "|   lol|33127|\n",
            "|thanks|31117|\n",
            "| going|30605|\n",
            "|  time|29182|\n",
            "| today|28409|\n",
            "|   got|27714|\n",
            "|   new|26527|\n",
            "|   one|25765|\n",
            "|   see|25378|\n",
            "|  know|25135|\n",
            "+------+-----+\n",
            "\n",
            "Top Words by Sentiment - Negative:\n",
            "+------+-----+\n",
            "|  word|count|\n",
            "+------+-----+\n",
            "|   get|45373|\n",
            "|  dont|45107|\n",
            "|  cant|43676|\n",
            "|  work|43427|\n",
            "|  like|40653|\n",
            "|   day|39279|\n",
            "| today|36157|\n",
            "| going|33423|\n",
            "|   got|33080|\n",
            "|  back|32439|\n",
            "|really|31167|\n",
            "|  miss|29993|\n",
            "|  want|29666|\n",
            "| still|28734|\n",
            "|  good|28499|\n",
            "+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da4b3c16",
        "outputId": "d3cbb692-7382-48cb-fa94-0d4e796c3cef"
      },
      "source": [
        "# Checking  Display a few examples from the raw 'date' column\n",
        "df.select(\"date\").show(5, truncate=False)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------------+\n",
            "|date                        |\n",
            "+----------------------------+\n",
            "|Mon Apr 06 22:19:45 PDT 2009|\n",
            "|Mon Apr 06 22:19:49 PDT 2009|\n",
            "|Mon Apr 06 22:19:53 PDT 2009|\n",
            "|Mon Apr 06 22:19:57 PDT 2009|\n",
            "|Mon Apr 06 22:19:57 PDT 2009|\n",
            "+----------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 5: Model Building and Evaluation\n",
        "\n",
        "print(\"\\nStep 5: Building sentiment classification models...\")\n",
        "model_results = analyzer.build_sentiment_classifier(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "hhozwU2-2IVB",
        "outputId": "2529c0bf-ff5a-4be8-ca2b-fb80c9b5e44d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 5: Building sentiment classification models...\n",
            "\n",
            "=== BUILDING SENTIMENT CLASSIFIER ===\n",
            "Training set: 1,247,647 tweets\n",
            "Test set: 311,426 tweets\n",
            "\n",
            "1. Training Logistic Regression Model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SparkUpgradeException",
          "evalue": "[INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEE MMM dd HH:mm:ss XXX yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSparkUpgradeException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1396649935.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nStep 5: Building sentiment classification models...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_sentiment_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3127855051.py\u001b[0m in \u001b[0;36mbuild_sentiment_classifier\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mlr_pipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremover\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhashingTF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mlr_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0mlr_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSparkUpgradeException\u001b[0m: [INCONSISTENT_BEHAVIOR_CROSS_VERSION.DATETIME_PATTERN_RECOGNITION] You may get a different result due to the upgrading to Spark >= 3.0:\nFail to recognize 'EEE MMM dd HH:mm:ss XXX yyyy' pattern in the DateTimeFormatter. 1) You can set \"spark.sql.legacy.timeParserPolicy\" to \"LEGACY\" to restore the behavior before Spark 3.0. 2) You can form a valid datetime pattern with the guide from 'https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 6: Advanced Analysis and Prediction\n",
        "\n",
        "print(\"\\nStep 6: Performing advanced sentiment analysis...\")\n",
        "advanced_results = analyzer.advanced_sentiment_analysis(df)\n",
        "\n",
        "print(\"\\nStep 7: Testing model with new tweets...\")\n",
        "test_tweets = [\n",
        "    \"I love this new product! It's amazing!\",\n",
        "    \"This is terrible service. Very disappointed.\",\n",
        "    \"The weather is nice today.\",\n",
        "    \"Best day ever! So happy right now!\",\n",
        "    \"Worst experience of my life. Avoid this place.\"\n",
        "]\n",
        "predictions = analyzer.predict_new_tweets(model_results['best_model'], test_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 922
        },
        "id": "1VqqaFcD2ISh",
        "outputId": "0ac0d99e-c790-405b-8092-389732518600"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step 6: Performing advanced sentiment analysis...\n",
            "\n",
            "=== ADVANCED SENTIMENT ANALYSIS ===\n",
            "1. Sentiment Patterns by Hour:\n",
            "+----+---------+------+\n",
            "|hour|sentiment| count|\n",
            "+----+---------+------+\n",
            "|NULL| negative|783531|\n",
            "|NULL| positive|775542|\n",
            "+----+---------+------+\n",
            "\n",
            "2. User Sentiment Consistency:\n",
            "Users with most diverse sentiment (both positive and negative):\n",
            "+---------------+---------------+------------+\n",
            "|           user|sentiment_types|total_tweets|\n",
            "+---------------+---------------+------------+\n",
            "|        webwoke|              2|         345|\n",
            "|    VioletsCRUK|              2|         276|\n",
            "|SallytheShizzle|              2|         272|\n",
            "|    mcraddictal|              2|         259|\n",
            "|       tsarnick|              2|         246|\n",
            "|      DarkPiano|              2|         226|\n",
            "|   SongoftheOss|              2|         220|\n",
            "|      Jayme1988|              2|         217|\n",
            "|         keza34|              2|         215|\n",
            "|    Karen230683|              2|         213|\n",
            "+---------------+---------------+------------+\n",
            "\n",
            "3. Sentiment by Tweet Length:\n",
            "+---------------+---------+------+\n",
            "|length_category|sentiment| count|\n",
            "+---------------+---------+------+\n",
            "|           long| negative|159924|\n",
            "|           long| positive|134009|\n",
            "|         medium| negative|318007|\n",
            "|         medium| positive|311912|\n",
            "|          short| negative|305600|\n",
            "|          short| positive|329621|\n",
            "+---------------+---------+------+\n",
            "\n",
            "\n",
            "Step 7: Testing model with new tweets...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_results' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3164618122.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"Worst experience of my life. Avoid this place.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m ]\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_new_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'best_model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_tweets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 7: Final Reporting and Cleanup\n",
        "\n",
        "# Step 8: Generate comprehensive report\n",
        "all_results = {**eda_results, **word_analysis, **model_results, **advanced_results}\n",
        "analyzer.generate_insights_report(df, model_results)\n",
        "\n",
        "# Step 9: Create visualizations\n",
        "analyzer.visualize_results({**eda_results, **word_analysis})\n",
        "\n",
        "# Step 10: Save results\n",
        "analyzer.save_results(df, all_results, output_path)\n",
        "\n",
        "print(\"\\n✅ Analysis completed successfully!\")\n",
        "print(f\"📊 Check '{output_path}' folder for detailed results\")\n",
        "print(\"📈 Visualizations saved as 'sentiment140_analysis.png'\")\n",
        "\n",
        "analyzer.close()"
      ],
      "metadata": {
        "id": "ychCBSkq3XTg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhE7KW_u3XP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YAgPHklf3XNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXc7JrNK3XLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lIpe6s4Q2IQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional utility functions for Sentiment140 dataset\n",
        "\n",
        "def download_sentiment140_dataset():\n",
        "    \"\"\"Instructions to download Sentiment140 dataset\"\"\"\n",
        "    instructions = \"\"\"\n",
        "    To download the Sentiment140 dataset:\n",
        "    \n",
        "    1. Go to: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
        "    2. Click 'Download' (requires Kaggle account)\n",
        "    3. Extract the ZIP file\n",
        "    4. The main file is: training.1600000.processed.noemoticon.csv\n",
        "    5. Update the dataset_path variable in main() function\n",
        "    \n",
        "    Alternative download via Kaggle API:\n",
        "    pip install kaggle\n",
        "    kaggle datasets download -d kazanova/sentiment140\n",
        "    \"\"\"\n",
        "    return instructions\n",
        "\n",
        "def sample_sentiment140_for_testing(input_path, output_path, sample_size=10000):\n",
        "    \"\"\"Create a smaller sample for testing purposes\"\"\"\n",
        "    analyzer = Sentiment140Analyzer(\"SampleCreator\")\n",
        "    \n",
        "    # Load full dataset\n",
        "    df = analyzer.load_sentiment140_dataset(input_path)\n",
        "    \n",
        "    # Take a random sample\n",
        "    sample_df = df.sample(fraction=sample_size/df.count(), seed=42)\n",
        "    \n",
        "    # Save sample\n",
        "    sample_df.write.mode(\"overwrite\").csv(output_path, header=True)\n",
        "    \n",
        "    print(f\"Sample of {sample_size} tweets saved to {output_path}\")\n",
        "    analyzer.close()\n",
        "\n",
        "# Cloud deployment specific configurations\n",
        "CLOUD_CONFIGS = {\n",
        "    \"aws_emr\": {\n",
        "        \"dataset_path\": \"s3://your-bucket/sentiment140/training.1600000.processed.noemoticon.csv\",\n",
        "        \"output_path\": \"s3://your-bucket/results/sentiment140/\",\n",
        "        \"spark_configs\": {\n",
        "            \"spark.executor.memory\": \"4g\",\n",
        "            \"spark.driver.memory\": \"2g\",\n",
        "            \"spark.executor.cores\": \"2\"\n",
        "        }\n",
        "    },\n",
        "    \"gcp_dataproc\": {\n",
        "        \"dataset_path\": \"gs://your-bucket/sentiment140/training.1600000.processed.noemoticon.csv\",\n",
        "        \"output_path\": \"gs://your-bucket/results/sentiment140/\",\n",
        "        \"spark_configs\": {\n",
        "            \"spark.executor.memory\": \"4g\",\n",
        "            \"spark.driver.memory\": \"2g\"\n",
        "        }\n",
        "    },\n",
        "    \"azure_hdinsight\": {\n",
        "        \"dataset_path\": \"abfss://container@account.dfs.core.windows.net/sentiment140/training.csv\",\n",
        "        \"output_path\": \"abfss://container@account.dfs.core.windows.net/results/sentiment140/\",\n",
        "        \"spark_configs\": {\n",
        "            \"spark.executor.memory\": \"4g\",\n",
        "            \"spark.driver.memory\": \"2g\"\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "7FQ8tzzD2C-s"
      }
    }
  ]
}